{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style= \"color :red\"> 자연어처리 </span>\n",
    "-  인간의 음성이나 텍스르를 컴퓨터가 인식하고 처리할 수 있도록 하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style= \"color :blue\"> 텍스트의 토큰화 </span>\n",
    "- 토큰 : 텍스트를 단어/문장/형태소 별로 나눈 하나의 단위\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  해보지 않으면 해낼 수 없다\n",
      "토큰화 :  ['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 전처리할 텍스트를 지정\n",
    "text = \"해보지 않으면 해낼 수 없다\"\n",
    "\n",
    "# 텍스트를 토큰화\n",
    "result = text_to_word_sequence(text)\n",
    "print(\"원문 : \",text)\n",
    "print(\"토큰화 : \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizer : 단어의 빈도수를 쉽게 계산할 수 있도록 하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 카운트 :\n",
      "   OrderedDict([('진짜', 1), ('원한', 1), ('해결사', 1), ('그', 1), ('자체', 1), ('고문도', 1), ('잔인해서', 1), ('속시원하고', 1), ('또', 1), ('특이하고', 1), ('자극적이고', 1), ('인기', 1), ('많은', 1), ('이유가', 1), ('있었구나ㅋㅋ', 1), ('어딜가던', 1), ('범죄자', 1), ('처벌', 1), ('맘에', 1), ('안든다고', 1), ('욕먹는데', 1), ('직접이렇게', 1), ('처리해주니', 1), ('개연성을', 1), ('떠나서', 1), ('중세시대', 1), ('고문기구같은것도보고', 1), ('자극적이지만', 1), ('못끊을거같', 1), ('이', 1), ('시리즈', 1), ('솔직히', 1), ('너무', 1), ('좋이요', 1), ('우리나라의', 1), ('약한', 1), ('처벌이랑', 1), ('반대되는것', 1), ('같아요', 1)])\n",
      "\n",
      "문장 카운트 :   3\n",
      "\n",
      "각 단어가 몇 개의 문장에 포함되었는가? :  \n",
      " defaultdict(<class 'int'>, {'자체': 1, '인기': 1, '고문도': 1, '자극적이고': 1, '있었구나ㅋㅋ': 1, '또': 1, '속시원하고': 1, '원한': 1, '이유가': 1, '많은': 1, '잔인해서': 1, '특이하고': 1, '해결사': 1, '진짜': 1, '그': 1, '범죄자': 1, '욕먹는데': 1, '맘에': 1, '자극적이지만': 1, '어딜가던': 1, '직접이렇게': 1, '못끊을거같': 1, '처벌': 1, '중세시대': 1, '고문기구같은것도보고': 1, '개연성을': 1, '처리해주니': 1, '떠나서': 1, '안든다고': 1, '좋이요': 1, '너무': 1, '솔직히': 1, '약한': 1, '시리즈': 1, '같아요': 1, '반대되는것': 1, '우리나라의': 1, '이': 1, '처벌이랑': 1})\n",
      "\n",
      "각 단어가 매겨진 인덱스 값 :  \n",
      " {'진짜': 1, '원한': 2, '해결사': 3, '그': 4, '자체': 5, '고문도': 6, '잔인해서': 7, '속시원하고': 8, '또': 9, '특이하고': 10, '자극적이고': 11, '인기': 12, '많은': 13, '이유가': 14, '있었구나ㅋㅋ': 15, '어딜가던': 16, '범죄자': 17, '처벌': 18, '맘에': 19, '안든다고': 20, '욕먹는데': 21, '직접이렇게': 22, '처리해주니': 23, '개연성을': 24, '떠나서': 25, '중세시대': 26, '고문기구같은것도보고': 27, '자극적이지만': 28, '못끊을거같': 29, '이': 30, '시리즈': 31, '솔직히': 32, '너무': 33, '좋이요': 34, '우리나라의': 35, '약한': 36, '처벌이랑': 37, '반대되는것': 38, '같아요': 39}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "docs = [\"진짜 원한 해결사 그 자체... 고문도 잔인해서 속시원하고 또 특이하고 자극적이고... 인기 많은 이유가 있었구나ㅋㅋ\",\n",
    "        \"어딜가던 범죄자 처벌 맘에 안든다고 욕먹는데 직접이렇게 처리해주니 개연성을 떠나서 중세시대 고문기구같은것도보고 자극적이지만 못끊을거같\",\n",
    "        \"이 시리즈 솔직히 너무 좋이요! 우리나라의 약한 처벌이랑 반대되는것 같아요\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)            # 리스트형태로 담아야함\n",
    "\n",
    "print(\"\\n단어 카운트 :\\n  \", tokenizer.word_counts)\n",
    "print(\"\\n문장 카운트 :  \", tokenizer.document_count)\n",
    "print(\"\\n각 단어가 몇 개의 문장에 포함되었는가? :  \\n\", tokenizer.word_docs)\n",
    "print(\"\\n각 단어가 매겨진 인덱스 값 :  \\n\", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style= \"color :blue\"> 각 단어의 원핫 인코딩 </span>\n",
    "- 단어사전의 크기만큼의 백터를 만들고 해당하는 단어의 위치에만 1을 두고 나머지 백터에는 0을 입력하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 인덱스 : \n",
      " {'휴먼버그대학교에서는': 1, '세계곳곳에서': 2, '실제로': 3, '일어난': 4, '사건들에': 5, '근거한': 6, '스토리를': 7, '재밌는': 8, '만화로': 9, '소개하고': 10, '있습니다': 11}\n",
      "\n",
      "토큰 인덱스로 이뤄진 새로운 문장 배열 :\n",
      " [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n",
      "\n",
      "원핫 인코딩 : \n",
      " [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "text = \"휴먼버그대학교에서는 세계곳곳에서 실제로 일어난 사건들에 근거한 스토리를 재밌는 만화로 소개하고 있습니다\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print(\"토큰 인덱스 : \\n\",tokenizer.word_index)\n",
    "\n",
    "\n",
    "x = tokenizer.texts_to_sequences([text])\n",
    "print(\"\\n토큰 인덱스로 이뤄진 새로운 문장 배열 :\\n\",x)\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 인덱스 수에 하나를 추가해서 원핫 인코딩 배열 생성\n",
    "word_size = len(tokenizer.word_index)+1\n",
    "x = to_categorical(x, num_classes = word_size)\n",
    "print (\"\\n원핫 인코딩 : \\n\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style= \"color :blue\"> 단어 임베딩 </span>\n",
    "- 원핫인코딩의 단점 : 메모리의 비효율성 (단어가 1만개이면 하나의 단어를 표현하기 위해 1만공간의 백터를 생성해야 함)\n",
    "- 임베딩 : 단어 간의 유사도를 계산하여 적절한 크기의 백터 배열로 만드는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(16,4))     # Embedding(입력될 총 단어의 수, 임베딩 후 출력될 백터의 크기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실습 : 문장의 부정/긍정을 판별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'영화에요': 1, '너무': 2, '재밌네요': 3, '최고에요': 4, '참': 5, '잘': 6, '만든': 7, '추천하고': 8, '싶은': 9, '한번': 10, '더': 11, '보고싶네요': 12, '글쎄요': 13, '별로에요': 14, '생각보다': 15, '지루하네요': 16, '연기가': 17, '어색해요': 18, '재미없어요': 19}\n",
      "\n",
      "리뷰 텍스트, 토큰화 결과 : \n",
      " [[2, 3], [4], [5, 6, 7, 1], [8, 9, 1], [10, 11, 12], [13], [14], [15, 16], [17, 18], [19]]\n",
      "\n",
      "패딩 결과 : \n",
      " [[ 0  0  2  3]\n",
      " [ 0  0  0  4]\n",
      " [ 5  6  7  1]\n",
      " [ 0  8  9  1]\n",
      " [ 0 10 11 12]\n",
      " [ 0  0  0 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0 15 16]\n",
      " [ 0  0 17 18]\n",
      " [ 0  0  0 19]]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "docs = [\"너무 재밌네요\",\"최고에요\",\"참 잘 만든 영화에요\",\"추천하고 싶은 영화에요\",\"한번 더 보고싶네요\",\n",
    "        \"글쎄요\",\"별로에요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
    "\n",
    "classes = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "\n",
    "# 문장 토큰화 \n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)\n",
    "\n",
    "# 텍스트를 단어의 인덱스로 변환 \n",
    "x = token.texts_to_sequences(docs)\n",
    "print(\"\\n리뷰 텍스트, 토큰화 결과 : \\n\", x)\n",
    "\n",
    "# 모델 학습을 위해 리뷰 길이를 맞춤 \n",
    "padded_x = pad_sequences(x, 4)      # 해당데이터를 4의 길이로 맞춤 \n",
    "print(\"\\n패딩 결과 : \\n\", padded_x)\n",
    "\n",
    "# 임베딩에 입력될 단어의 수를 지정 \n",
    "word_size = len(token.word_index)+1\n",
    "print(word_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 4, 8)              160       \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6512 - accuracy: 0.8000\n",
      "모델 정확도 :  0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "# 단어 임베딩을 포함해 딥러닝 모델을 만들고 결과를 출력 \n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(word_size, 8, input_length=4))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics=\"accuracy\")\n",
    "model.fit(padded_x, classes, epochs=20, verbose=0)\n",
    "print(\"모델 정확도 : \",model.evaluate(padded_x,classes)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8df4e0e0c796e2e15924145e6441c62a115ba500877f31d29fc14fd3395fcd67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
